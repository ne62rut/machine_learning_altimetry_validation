{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2120687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "import netCDF4\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import random\n",
    "\n",
    "#import neighbour_functions\n",
    "from haversine_distance import haversine_distance\n",
    "#from make_data_features import make_data_features\n",
    "# from neighbour_functions import compute_neighbours_stats\n",
    "# from neighbour_functions import add_nearby_observations\n",
    "# from neighbour_functions import additional_stats\n",
    "\n",
    "import cartopy as cart\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14285c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neighbours_stats(selection,distance,time_stamp,criterion) :\n",
    "\n",
    "    r = 2\n",
    "    \n",
    "    if criterion == 'distance':\n",
    "    \n",
    "        #Find points no further than 250 km\n",
    "        selection_neighbour_one = selection.loc[( (selection['distance'] < distance) & (selection['distance'] >=1)) ]\n",
    "        #selection_neighbour_one = selection.loc[ (selection['distance'] < distance)  ]\n",
    "\n",
    "        # Put selection in ascending order based on distance\n",
    "        selection_neighbour_one = selection_neighbour_one.sort_values(['distance'])\n",
    "        \n",
    "    elif criterion == 'time':\n",
    "        \n",
    "        #Find points no further than some days\n",
    "        time_diff_selection = abs(selection.date-pd.to_datetime(time_stamp)).dt.total_seconds()+1 #(one second is added to avoid infinite values for observations taken in the same hour)\n",
    "        selection_neighbour_one = selection.loc[( (time_diff_selection < distance) & (selection['distance'] >=1) & (selection['distance'] <=300) ) ]\n",
    "        # Put selection in ascending order based on distance\n",
    "        selection_neighbour_one = selection_neighbour_one.sort_values(['distance'])    \n",
    "\n",
    "    # Space weighting coefficient\n",
    "    lambda_coeff_one = selection_neighbour_one.distance**(-r)  / np.sum(selection_neighbour_one.distance**(-r))\n",
    "    #lambda_coeff_one = 1./( (selection_neighbour_one.distance*0.01)**(r) ) \n",
    "    \n",
    "\n",
    "    # Time weighting coefficient    \n",
    "    time_diff = (abs(selection_neighbour_one.date-pd.to_datetime(time_stamp)).dt.total_seconds()+1)*10**(-4) \n",
    "    #(one second is added to avoid infinite values for observations taken in the same hour)\n",
    "    #Time difference (in seconds) is multiplied by the power of -4 in order to have similar numbers to space distances (in Km)\n",
    "    lambda_coeff_two = time_diff**(-r)  / np.sum(time_diff**(-r))\n",
    "\n",
    "\n",
    "    sla_weighted_space = np.sum   (lambda_coeff_one *  selection_neighbour_one.sla )\n",
    "    #sla_weighted_space = sum   (lambda_coeff_one *  selection_neighbour_one.sla ) / sum   (lambda_coeff_one)\n",
    "    \n",
    "    sla_weighted_time = np.sum   (lambda_coeff_two *  selection_neighbour_one.sla )\n",
    "    sla_spread = np.nanstd(selection_neighbour_one.sla)\n",
    "    sla_average = np.nanmean(selection_neighbour_one.sla)       \n",
    "    \n",
    "    return selection_neighbour_one, sla_weighted_space, sla_weighted_time, sla_spread, sla_average\n",
    "\n",
    "def add_nearby_observations(X_new,X,y):\n",
    "    \n",
    "    #X_new is the dataframe that needs to add the closest values as attributes\n",
    "    #X is the dataframe in which to find the closest values\n",
    "    #y is the dataframe containing the truth\n",
    "    \n",
    "    \n",
    "    # NEIGHBOUR ONE\n",
    "    \n",
    "    z_k1 = np.empty(np.shape(X_new)[0])  #sla average\n",
    "    z_tilde_k1 = np.empty(np.shape(X_new)[0]) #sla weighted average in space\n",
    "    z_tildetime_k1 = np.empty(np.shape(X_new)[0]) #sla weighted average in space\n",
    "    sigma_k1 = np.empty(np.shape(X_new)[0])  #sla spread\n",
    "    \n",
    "    z_u1 = np.empty(np.shape(X_new)[0])  #sla average\n",
    "    z_tilde_u1 = np.empty(np.shape(X_new)[0]) #sla weighted average in space\n",
    "    z_tildetime_u1 = np.empty(np.shape(X_new)[0]) #sla weighted average in space\n",
    "    sigma_u1 = np.empty(np.shape(X_new)[0])  #sla spread    \n",
    "    \n",
    "    \n",
    "    # NEIGHBOUR TWO\n",
    "    \n",
    "    z_k2 = np.empty(np.shape(X_new)[0])  #sla average\n",
    "    z_tilde_k2 = np.empty(np.shape(X_new)[0]) #sla weighted average in space\n",
    "    z_tildetime_k2 = np.empty(np.shape(X_new)[0]) #sla weighted average in space\n",
    "    sigma_k2 = np.empty(np.shape(X_new)[0])  #sla spread   \n",
    "    \n",
    "    z_u2 = np.empty(np.shape(X_new)[0])  #sla average\n",
    "    z_tilde_u2 = np.empty(np.shape(X_new)[0]) #sla weighted average in space\n",
    "    z_tildetime_u2 = np.empty(np.shape(X_new)[0]) #sla weighted average in space\n",
    "    sigma_u2 = np.empty(np.shape(X_new)[0])  #sla spread       \n",
    "    \n",
    "    # NEIGHBOUR Three\n",
    "    \n",
    "    z_k3 = np.empty(np.shape(X_new)[0])  #sla average\n",
    "    z_tilde_k3 = np.empty(np.shape(X_new)[0]) #sla weighted average in space\n",
    "    z_tildetime_k3 = np.empty(np.shape(X_new)[0]) #sla weighted average in space\n",
    "    sigma_k3 = np.empty(np.shape(X_new)[0])  #sla spread  \n",
    "    \n",
    "    z_u3 = np.empty(np.shape(X_new)[0])  #sla average\n",
    "    z_tilde_u3 = np.empty(np.shape(X_new)[0]) #sla weighted average in space\n",
    "    z_tildetime_u3 = np.empty(np.shape(X_new)[0]) #sla weighted average in space\n",
    "    sigma_u3 = np.empty(np.shape(X_new)[0])  #sla spread      \n",
    "    \n",
    "    \n",
    "    #Add datetime object to X\n",
    "    temp = X.drop(columns=['elevation','lon','lat'])\n",
    "    temp = pd.to_datetime(temp)\n",
    "    X['date'] = temp\n",
    "    \n",
    "\n",
    "\n",
    "    index = random.choice (np.arange(0,np.shape(X_new)[0])) \n",
    "    \n",
    "    #datetime of current point \n",
    "    time_stamp = datetime(int(X_new.iloc[index].year),int(X_new.iloc[index].month),int(X_new.iloc[index].day),int(X_new.iloc[index].hour))\n",
    "\n",
    "\n",
    "    #Find points within a certain time and space bound, but not from the same track at the same cycle \n",
    "    selection = X.loc[ (abs((X['date'] - time_stamp ).dt.days)< 30) & \\\n",
    "                      ( abs((X['date'] - time_stamp ).dt.seconds)> 3600 ) & \\\n",
    "                     (abs((X['lat'] - X_new.iloc[index].lat))< 2.5) ]  \n",
    "\n",
    "    #Add distance from point\n",
    "    selection['distance'] = haversine_distance(X_new.iloc[index].lat, X_new.iloc[index].lon, selection.lat, selection.lon)\n",
    "\n",
    "    #Add SLA of selected points\n",
    "    selection['sla'] = y[y.index.isin(selection.index)]\n",
    "\n",
    "\n",
    "    # LETTER k IDENTIFIES THE NEIGHBOUROODS IN SPACE\n",
    "\n",
    "    selection_neighbour_one_100, z_tilde_k1[index], z_tildetime_k1[index], sigma_k1[index], z_k1[index] = \\\n",
    "                compute_neighbours_stats(selection,100,time_stamp,'distance')\n",
    "\n",
    "    selection_neighbour_one_200, z_tilde_k2[index], z_tildetime_k2[index], sigma_k2[index], z_k2[index] = \\\n",
    "                compute_neighbours_stats(selection,200,time_stamp,'distance')\n",
    "\n",
    "    selection_neighbour_one_300, z_tilde_k3[index], z_tildetime_k3[index], sigma_k3[index], z_k3[index] = \\\n",
    "                compute_neighbours_stats(selection,300,time_stamp,'distance')\n",
    "\n",
    "    # LETTER u IDENTIFIES THE NEIGHBOUROODS IN TIME\n",
    "\n",
    "    selection_neighbour_one_5, z_tilde_u1[index], z_tildetime_u1[index], sigma_u1[index], z_u1[index] = \\\n",
    "                compute_neighbours_stats(selection,5*86400,time_stamp,'time')\n",
    "\n",
    "    selection_neighbour_one_15, z_tilde_u2[index], z_tildetime_u2[index], sigma_u2[index], z_u2[index] = \\\n",
    "                compute_neighbours_stats(selection,15*86400,time_stamp,'time')\n",
    "\n",
    "    selection_neighbour_one_30, z_tilde_u3[index], z_tildetime_u3[index], sigma_u3[index], z_u3[index] = \\\n",
    "                compute_neighbours_stats(selection,30*86400,time_stamp,'time')        \n",
    "    \n",
    "    X = X.drop(columns=['date'])\n",
    "\n",
    "    # Addition of statistics relative to a single neighbour\n",
    "\n",
    "    X_new['z_k1'] = z_k1\n",
    "    X_new['z_tilde_k1'] = z_tilde_k1\n",
    "    X_new['z_tildetime_k1'] = z_tildetime_k1\n",
    "    X_new['sigma_k1'] = sigma_k1     \n",
    "    \n",
    "    X_new['z_k2'] = z_k2\n",
    "    X_new['z_tilde_k2'] = z_tilde_k2\n",
    "    X_new['z_tildetime_k2'] = z_tildetime_k2\n",
    "    X_new['sigma_k2'] = sigma_k2 \n",
    "    \n",
    "    X_new['z_k3'] = z_k3\n",
    "    X_new['z_tilde_k3'] = z_tilde_k3\n",
    "    X_new['z_tildetime_k3'] = z_tildetime_k3\n",
    "    X_new['sigma_k3'] = sigma_k3 \n",
    "    \n",
    "    X_new['z_u1'] = z_u1\n",
    "    X_new['z_tilde_u1'] = z_tilde_u1\n",
    "    X_new['z_tildetime_u1'] = z_tildetime_u1\n",
    "    X_new['sigma_u1'] = sigma_u1     \n",
    "    \n",
    "    X_new['z_u2'] = z_u2\n",
    "    X_new['z_tilde_u2'] = z_tilde_u2\n",
    "    X_new['z_tildetime_u2'] = z_tildetime_u2\n",
    "    X_new['sigma_u2'] = sigma_u2 \n",
    "    \n",
    "    X_new['z_u3'] = z_u3\n",
    "    X_new['z_tilde_u3'] = z_tilde_u3\n",
    "    X_new['z_tildetime_u3'] = z_tildetime_u3\n",
    "    X_new['sigma_u3'] = sigma_u3 \n",
    "    \n",
    "\n",
    "    return selection_neighbour_one_100, selection_neighbour_one_200, selection_neighbour_one_300, \\\n",
    "selection_neighbour_one_5, selection_neighbour_one_15, selection_neighbour_one_30, X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfd03cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose region #BALTIC or #NORTHSEA\n",
    "region = 'NORTHSEA'\n",
    "\n",
    "#Choose Data Location\n",
    "location = 'cmems'\n",
    "\n",
    "#Choose year to consider\n",
    "year_of_choice = 2004\n",
    "\n",
    "# Choose how larger the dataset should be w.r.t. the prediction zone (in degrees of latitude and longitude)\n",
    "enlarge_dataset = 2.5\n",
    "\n",
    "if region in {'BALTIC'} :\n",
    "    max_lat = 66.0\n",
    "    min_lat = 53.0\n",
    "    max_lon = 31.0\n",
    "    min_lon = 9.0\n",
    "    \n",
    "    max_lat_large = 66.0 + enlarge_dataset\n",
    "    min_lat_large = 53.0 - enlarge_dataset\n",
    "    max_lon_large = 31.0 + enlarge_dataset\n",
    "    min_lon_large = 9.0  - enlarge_dataset    \n",
    "     \n",
    "    \n",
    "    #Name of the dataset containing altimetry observations\n",
    "    altimetry_dataset = \"/DGFI8/H/work_marcello/machine_learning_altimetry/ds_full_newtraining.pkl\"\n",
    "\n",
    "elif region in {'NORTHSEA'} :\n",
    "    \n",
    "    max_lat = 61.0\n",
    "    min_lat = 50.0\n",
    "    max_lon = 12.2\n",
    "    min_lon = -4.0\n",
    "    \n",
    "    max_lat_large = 61.0 + enlarge_dataset\n",
    "    min_lat_large = 50.0 - enlarge_dataset\n",
    "    max_lon_large = 12.2 + enlarge_dataset\n",
    "    min_lon_large = -4.0  - enlarge_dataset      \n",
    "    \n",
    "    \n",
    "    #Name of the dataset containing altimetry observations\n",
    "    altimetry_dataset = \"/DGFI8/H/work_marcello/machine_learning_altimetry/ds_full_newtraining_NORTHSEA.pkl\"    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7873f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe containing altimetry observations\n",
    "\n",
    "if location in {'cmems'} :\n",
    "    ds_full = pd.read_pickle(altimetry_dataset)\n",
    "    #CASE CMEMS\n",
    "    ds_full = ds_full.rename(columns = {'longitude': 'lon', 'latitude': 'lat', 'sla_unfiltered': 'sla'}, inplace = False)\n",
    "\n",
    "elif location in {'balticplus'} :\n",
    "    ds_full = pd.read_pickle(altimetry_dataset)\n",
    "    #CASE Baltic plus\n",
    "    ds_full = ds_full.rename(columns = {'sla_unfiltered': 'sla'}, inplace = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c1c20",
   "metadata": {},
   "source": [
    "Transform the xarray dataset into pandas dataframe. A possibility to convert longitude and latitude coordinates into trigonometric functions is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b614de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_data_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5260ad4f9a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdata_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_data_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_full\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfield_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_data_features' is not defined"
     ]
    }
   ],
   "source": [
    "def convert_coordinates(ds_full):\n",
    "\n",
    "    ds_full['A'] = np.sin(np.radians(ds_full.lat.values)) \n",
    "    ds_full['B'] = np.sin(np.radians(ds_full.lon.values)) * np.cos(np.radians(ds_full.lat.values))\n",
    "    ds_full['C'] = -np.cos(np.radians(ds_full.lon.values)) * np.cos(np.radians(ds_full.lat.values))\n",
    "    \n",
    "    return ds_full\n",
    "\n",
    "\n",
    "\n",
    "#Activate this to convert latitude and longitude\n",
    "#ds_full = convert_coordinates(ds_full)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "field_names = [  'elevation','lon','lat','year','month','day','hour']\n",
    "\n",
    "\n",
    "data_features, X = make_data_features(ds_full,field_names)\n",
    "\n",
    "\n",
    "data_target = {'sla': ds_full.sla}\n",
    "y = pd.DataFrame(data=data_target)\n",
    "y.columns =['sla'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824b10c",
   "metadata": {},
   "source": [
    "Possibilities to reduce size of training dataset + save a copy of the original altimetry dataset to be used for training (X and y), before any modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85130d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of specific years\n",
    "X = X.loc[ (X['year'] == year_of_choice)  ]\n",
    "y = y[y.index.isin(X.index)]\n",
    "\n",
    "# Save X as it is now, i.e. with larger areas compared to the training and prediction points\n",
    "X_original = X.copy()\n",
    "y_original = y.copy()\n",
    "\n",
    "# # Downsampling\n",
    "# X = X.iloc[::20]\n",
    "# y = y.iloc[::20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1afdf1",
   "metadata": {},
   "source": [
    "Regional exclusions (restrict area to training and prediction points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf67f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if region in {'BALTIC'} :\n",
    "    # Eliminate Skagerrak and Kattegat\n",
    "    X = X.loc[ ~( (X.lon < 13.0) & (X.lon > 9) &   \\\n",
    "                              (X.lat < 60.0) & (X.lat > 55.0) )]\n",
    "\n",
    "    # Eliminate North-West Norway angle\n",
    "    X = X.loc[ ~( (X.lon < 12.3)  )]\n",
    "    \n",
    "X = X.loc[ ( (X.lon < max_lon) & (X.lon > min_lon) &   \\\n",
    "                              (X.lat < max_lat) & (X.lat > min_lat) )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbceafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "from importlib import reload\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94538d03",
   "metadata": {},
   "source": [
    "Add neighbouring observations to Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d17f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one day of grid\n",
    "groups = X.groupby(['year','month','day'])\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for group_key, group_value in groups:\n",
    "    group = groups.get_group(group_key)\n",
    "    y_group = y.loc[group.index]\n",
    "\n",
    "    print(group_key)\n",
    "\n",
    "\n",
    "    from importlib import reload\n",
    "    reload(neighbour_functions)\n",
    "\n",
    "    # Create neighboroods\n",
    "    try:\n",
    "        X_original=X_original.drop(columns=['date'])\n",
    "        group=group.drop(columns=['date'])\n",
    "        X=X.drop(columns=['date'])            \n",
    "    except:\n",
    "        print()\n",
    "\n",
    "    selection_neighbour_one_100, selection_neighbour_one_200, selection_neighbour_one_300, \\\n",
    "selection_neighbour_one_5, selection_neighbour_one_15, selection_neighbour_one_30,group = add_nearby_observations(group,X_original,y_original)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8566f48c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 10))\n",
    "\n",
    "counter = 1\n",
    "ax=fig.add_subplot(1,2,counter,projection=ccrs.PlateCarree())\n",
    "\n",
    "ax.set_xticks(np.arange(min_lon_large,max_lon_large,2), crs=ccrs.PlateCarree())\n",
    "ax.set_yticks(np.arange(min_lat_large,max_lat_large,1), crs=ccrs.PlateCarree())    \n",
    "\n",
    "#ax = plt.axes(projection=ccrs.Miller()) \n",
    "img_three=plt.scatter(selection_neighbour_one_300.lon, selection_neighbour_one_300.lat, s=40, alpha=1,color='salmon')\n",
    "img_two=plt.scatter(selection_neighbour_one_200.lon, selection_neighbour_one_200.lat, s=40, alpha=1,color='gold')\n",
    "img=plt.scatter(selection_neighbour_one_100.lon, selection_neighbour_one_100.lat, s=40, alpha=1,color='royalblue')\n",
    "ax.coastlines(resolution='10m', color='black', linewidth=1) \n",
    "plt.xlim(min_lon_large,max_lon_large)\n",
    "plt.ylim(min_lat_large,max_lat_large)\n",
    "\n",
    "lon_formatter = cart.mpl.ticker.LongitudeFormatter(number_format='.1f', \n",
    "                                  degree_symbol='', \n",
    "                                  dateline_direction_label=True)\n",
    "lat_formatter = cart.mpl.ticker.LatitudeFormatter(number_format='.1f', \n",
    "                                 degree_symbol='')\n",
    "ax.xaxis.set_major_formatter(lon_formatter)\n",
    "ax.yaxis.set_major_formatter(lat_formatter) \n",
    "\n",
    "plt.gca().add_patch(Rectangle((min_lon,min_lat),abs(max_lon-min_lon),abs(max_lat-min_lat),\n",
    "                    edgecolor='red',\n",
    "                    facecolor='none',\n",
    "                    lw=4))\n",
    "\n",
    "#-----------\n",
    "\n",
    "counter = counter+1\n",
    "\n",
    "ax=fig.add_subplot(1,2,counter,projection=ccrs.PlateCarree())\n",
    "\n",
    "ax.set_xticks(np.arange(min_lon_large,max_lon_large,2), crs=ccrs.PlateCarree())\n",
    "ax.set_yticks(np.arange(min_lat_large,max_lat_large,1), crs=ccrs.PlateCarree())    \n",
    "\n",
    "#ax = plt.axes(projection=ccrs.Miller()) \n",
    "img_three=plt.scatter(selection_neighbour_one_30.lon, selection_neighbour_one_30.lat, s=40, alpha=1,color='salmon')\n",
    "img_two=plt.scatter(selection_neighbour_one_15.lon, selection_neighbour_one_15.lat, s=40, alpha=1,color='gold')\n",
    "img=plt.scatter(selection_neighbour_one_5.lon, selection_neighbour_one_5.lat, s=40, alpha=1,color='royalblue')\n",
    "ax.coastlines(resolution='10m', color='black', linewidth=1) \n",
    "plt.xlim(min_lon_large,max_lon_large)\n",
    "plt.ylim(min_lat_large,max_lat_large)\n",
    "\n",
    "lon_formatter = cart.mpl.ticker.LongitudeFormatter(number_format='.1f', \n",
    "                                  degree_symbol='', \n",
    "                                  dateline_direction_label=True)\n",
    "lat_formatter = cart.mpl.ticker.LatitudeFormatter(number_format='.1f', \n",
    "                                 degree_symbol='')\n",
    "ax.xaxis.set_major_formatter(lon_formatter)\n",
    "ax.yaxis.set_major_formatter(lat_formatter) \n",
    "\n",
    "\n",
    "plt.gca().add_patch(Rectangle((min_lon,min_lat),abs(max_lon-min_lon),abs(max_lat-min_lat),\n",
    "                    edgecolor='red',\n",
    "                    facecolor='none',\n",
    "                    lw=4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21caa9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_learning_altimetry]",
   "language": "python",
   "name": "conda-env-machine_learning_altimetry-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
