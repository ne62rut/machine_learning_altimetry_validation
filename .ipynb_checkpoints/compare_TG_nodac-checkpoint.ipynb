{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cartopy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-00d5830acbae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#from oceans.filters import lanc  #pip install ocean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcartopy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcartopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLongitudeFormatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLatitudeFormatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcartopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mccrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cartopy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "import netCDF4\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "#from oceans.filters import lanc  #pip install ocean\n",
    "import cartopy as cart\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_pass_weights(window, cutoff):\n",
    "    \"\"\"Calculate weights for a low pass Lanczos filter.\n",
    "    Args:\n",
    "    window: int\n",
    "        The length of the filter window.\n",
    "    cutoff: float\n",
    "        The cutoff frequency in inverse time steps.\n",
    "    \"\"\"\n",
    "    order = ((window - 1) // 2 ) + 1\n",
    "    nwts = 2 * order + 1\n",
    "    w = np.zeros([nwts])\n",
    "    n = nwts // 2\n",
    "    w[n] = 2 * cutoff\n",
    "    k = np.arange(1., n)\n",
    "    sigma = np.sin(np.pi * k / n) * n / (np.pi * k)\n",
    "    firstfactor = np.sin(2. * np.pi * cutoff * k) / (np.pi * k)\n",
    "    w[n-1:0:-1] = firstfactor * sigma\n",
    "    w[n+1:-1] = firstfactor * sigma\n",
    "    return w[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/nfs/public_ads/Oelsmann/marcello/gesla_v2/gesla_v2_northsea_balticsea.nc\n",
    "\n",
    "\n",
    "region = 'NORTHSEA'\n",
    "\n",
    "start_time = '01/01/2021'\n",
    "\n",
    "end_time = '31/12/2021'\n",
    "\n",
    "year = '2021'\n",
    "\n",
    "if region in {'BALTIC'} :\n",
    "    max_lat = 66.0\n",
    "    min_lat = 53.0\n",
    "    max_lon = 31.0\n",
    "    min_lon = 9.0\n",
    "    \n",
    "    #tg_dac = xr.open_dataset('/nfs/DGFI8/H/work_marcello/giussani_machinelearning_data/TG_DACcorrected/baltic_tg_gesla_dac.nc')\n",
    "    tg_dac = xr.open_dataset('/nfs/public_ads/Oelsmann/marcello/gesla_v2/gesla_v2_northsea_balticsea.nc')\n",
    "\n",
    "    lon = np.asarray(tg_dac.lon.values,dtype=float)\n",
    "    lat = np.asarray(tg_dac.lat.values,dtype=float)    \n",
    "    \n",
    "#     # Exclude tide gauges in Kattegat and Skagerrag, because out of the training area\n",
    "\n",
    "    indices =  ~( (lon < 13.0) & (lon > 9) &   \\\n",
    "                              (lat < 66.0) & (lat > 56.0)) & ((lon < max_lon) & (lon > min_lon) &   \\\n",
    "                              (lat < max_lat) & (lat > min_lat))   \n",
    "    \n",
    "\n",
    "    # Altimetry\n",
    "    alti_full = pd.read_csv(r'/DGFI8/H/work_marcello/machine_learning_altimetry/test_prediction_newpoints_surge_baltic2004_01to12.csv',index_col=[0],parse_dates=[0])\n",
    "    \n",
    "    #model\n",
    "    copernicus_full = pd.read_csv('/DGFI8/H/work_marcello/machine_learning_altimetry/test_prediction_newpoints_surge_copernicus.csv',index_col=[7],parse_dates=[7])\n",
    "    copernicus_full['sla_predicted'] = copernicus_full['sla_predicted'] \n",
    "\n",
    "    #cmems\n",
    "    cmems_full = pd.read_csv('/DGFI8/H/work_marcello/machine_learning_altimetry/test_prediction_newpoints_surge_cmems_allsat.csv',index_col=[5],parse_dates=[5])    \n",
    "    \n",
    "\n",
    "elif region in {'NORTHSEA'} :\n",
    "    \n",
    "    max_lat = 61.0\n",
    "    min_lat = 50.0\n",
    "    max_lon = 12.2\n",
    "    min_lon = -4.0  \n",
    "    \n",
    "    tg_dac = xr.open_dataset('/nfs/public_ads/Oelsmann/marcello/gesla_v3/gesla_2021_selected_JTEX_lowess_dac_correctedupdate.nc')\n",
    "    \n",
    "    lon = np.asarray(tg_dac.lon.values,dtype=float)\n",
    "    lat = np.asarray(tg_dac.lat.values,dtype=float)\n",
    "    \n",
    "    indices =  ( (lon < max_lon) & (lon > min_lon) &   \\\n",
    "                              (lat < max_lat) & (lat > min_lat) )    \n",
    "\n",
    "    # Altimetry\n",
    "    alti_full = pd.read_csv(r'/DGFI8/H/work_marcello/machine_learning_altimetry/test_prediction_newpoints_surge_northsea2021_01to12.csv',index_col=[0],parse_dates=[0])\n",
    "    \n",
    "    #model\n",
    "    copernicus_full = pd.read_csv('/DGFI8/H/work_marcello/machine_learning_altimetry/test_prediction_newpoints_surge_copernicus.csv',index_col=[7],parse_dates=[7])\n",
    "    copernicus_full['sla_predicted'] = copernicus_full['sla_predicted'] \n",
    "\n",
    "    #cmems\n",
    "    cmems_full = pd.read_csv('/DGFI8/H/work_marcello/machine_learning_altimetry/test_prediction_newpoints_surge_cmems_NORTHSEA_allsat.csv',index_col=[5],parse_dates=[5])\n",
    "\n",
    "\n",
    "\n",
    "tg_dac = tg_dac.sel({'x':indices })\n",
    "\n",
    "tg_dac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance vs TG\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    r = 6371\n",
    "    phi1 = np.radians(lat1)\n",
    "    \n",
    "    \n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 -lon1)\n",
    "    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) *   np.sin(delta_lambda / 2)**2\n",
    "    res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n",
    "\n",
    "    return np.round(res, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE PAPER\n",
    "\n",
    "# list_names = ['landsortnorra-010-sweden-smhi','gedser-gedser-denmark-dmi',\\\n",
    "#               'visby-026-sweden-smhi']\n",
    "\n",
    "#list_names = ['skanor-019-sweden-smhi','kungsholmsfort-008-sweden-smhi',\\\n",
    "#              'simrishamn-017-sweden-smhi']\n",
    "\n",
    "list_names = ['skanor-019-sweden-smhi']\n",
    "\n",
    "corr_ML=np.empty(np.size(tg_dac.x))*np.nan\n",
    "corr_cmems=np.empty(np.size(tg_dac.x))*np.nan\n",
    "\n",
    "rms_ML=np.empty(np.size(tg_dac.x))*np.nan\n",
    "rms_cmems=np.empty(np.size(tg_dac.x))*np.nan\n",
    "\n",
    "std_ML=np.empty(np.size(tg_dac.x))*np.nan\n",
    "std_cmems=np.empty(np.size(tg_dac.x))*np.nan\n",
    "std_tg=np.empty(np.size(tg_dac.x))*np.nan\n",
    "\n",
    "\n",
    "row_index = np.array([0,0,0])\n",
    "#col_index = np.array([0,1,2])\n",
    "col_index = np.arange(0,np.size(tg_dac.x))\n",
    "\n",
    "fig, axs = plt.subplots(np.size(tg_dac.x[0:4]), 1, sharex=True, sharey=False, figsize=(12,12))\n",
    "\n",
    "#fig_b, axs_b = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(12,12))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "\n",
    "#for tide_gauge_name in list_names :\n",
    "\n",
    "for count_TG in tg_dac.x[[0,3,13,20]] :\n",
    "\n",
    "\n",
    "    start_lat = np.asarray(tg_dac.lat[count_TG].values,dtype=float)\n",
    "    start_lon = np.asarray(tg_dac.lon[count_TG].values,dtype=float)\n",
    "\n",
    "    tide_gauge_name = tg_dac.index[count_TG]\n",
    "    tg_due = tg_dac\n",
    "    \n",
    "\n",
    "    # initialise data of lists.\n",
    "#     if region in {'BALTIC'} :\n",
    "#         data = {'time':tg_due.time.values,\n",
    "#                 'sla_dac':np.squeeze(tg_due.sla_dac.values)}\n",
    "#     else:\n",
    "    data = {'time':tg_due.time.values,\n",
    "            'sla_dac':tg_due.sla[:,count_TG]}    \n",
    "    #break\n",
    "\n",
    "    # Create DataFrame\n",
    "    tg = pd.DataFrame(data)\n",
    "    \n",
    "    # Remove mean from the tide gauge\n",
    "    mean_tg_due= tg[\"sla_dac\"].mean()\n",
    "    tg.sla_dac = tg.sla_dac - tg[\"sla_dac\"].mean()    \n",
    "    median_tg_due= tg[\"sla_dac\"].median()\n",
    "    mad_tg_due= tg[\"sla_dac\"].mad()    \n",
    "    \n",
    "    # Remove outliers from the tide gauge    \n",
    "    tg.loc[tg[\"sla_dac\"] > median_tg_due+1.4826*2*mad_tg_due] = np.nan\n",
    "    tg.loc[tg[\"sla_dac\"] < median_tg_due-1.4826*2*mad_tg_due] = np.nan\n",
    "    \n",
    "    tg=tg.set_index(tg.time)\n",
    "    tg=tg.loc[year+'-01-01':year+'-12-31']\n",
    "    \n",
    "    tg = tg.groupby(pd.Grouper(freq='24H')).mean()\n",
    "\n",
    "    #means_tg = tg.groupby(pd.Grouper(freq='3H')).mean()\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    #alti\n",
    "    alti=alti_full.loc[year+'-01-15':year+'-12-15']\n",
    "    cmems=cmems_full.loc[year+'-01-15':year+'-12-15']\n",
    "    \n",
    "    \n",
    "\n",
    "    # Compute distances from tide gauges \n",
    "    \n",
    "    distances_km = haversine_distance(start_lat, start_lon, alti.lat, alti.lon)\n",
    "    distances_km_cmems = haversine_distance(start_lat, start_lon, cmems.lat, cmems.lon)\n",
    "\n",
    "    alti['distances_km']=distances_km\n",
    "    cmems['distances_km']=distances_km_cmems\n",
    "\n",
    "    alti.dropna(subset = [\"sla_predicted\"], inplace=True)\n",
    "    cmems.dropna(subset = [\"sla_predicted\"], inplace=True)\n",
    "    \n",
    "\n",
    "    #alti = alti[distances_km<radius_of_distance_km]\n",
    "    #cmems = cmems[distances_km_cmems<radius_of_distance_km]\n",
    "    \n",
    "    \n",
    "    alti = alti.loc[alti['distances_km'] == np.nanmin(alti['distances_km'])]\n",
    "    cmems = cmems.loc[cmems['distances_km'] == np.nanmin(cmems['distances_km'])]\n",
    "    #cmems = cmems.loc[np.abs(cmems['distances_km'] - np.nanmin(alti['distances_km']))<5]\n",
    "    \n",
    "    alti = alti.groupby(['time']).mean()\n",
    "    cmems = cmems.groupby(['time']).mean()\n",
    "    \n",
    "\n",
    "#     if np.shape(tg.sla_dac.values[np.isnan(tg.sla_dac.values)])[0] > 60  :\n",
    "#         if counter == 0 :\n",
    "#             fig.legend()        \n",
    "#         counter = counter + 1\n",
    "#         continue        \n",
    "    \n",
    "    \n",
    "    freq = 1./10  # Hours\n",
    "    window_size = 60\n",
    "\n",
    "\n",
    "    # Plot of the products\n",
    "    try:\n",
    "        ref = alti.sla_predicted.plot(ax=axs[col_index[counter]],label=\"ML\",title='lon='+str(tide_gauge_name.lon.values)+' , '+'lat='+str(tide_gauge_name.lat.values),color='blue')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cmems.sla_predicted.plot(ax=axs[col_index[counter]],label=\"CMEMS\",color='orange')\n",
    "    except:\n",
    "        pass\n",
    "    tg.sla_dac.plot(ax=axs[col_index[counter]],label=\"Tide Gauge\",color='green')  \n",
    "    \n",
    "    ref.text(year+'-06-01',-0.25,'RMSE ML vs TG (m) = '+str( np.round( np.sqrt(((alti[\"sla_predicted\"] - tg[\"sla_dac\"]) ** 2).mean()),3 ) ) )\n",
    "    ref.text(year+'-06-01',-0.2,'RMSE CMEMS vs TG (m) = '+str( np.round( np.sqrt(((cmems[\"sla_predicted\"] - tg[\"sla_dac\"]) ** 2).mean()),3 ) ) )\n",
    "    \n",
    "    ref.set_ylabel(\"SLA (m)\")\n",
    "    \n",
    "    ref.set_ylim(-0.3, 0.3)\n",
    "    #ax.set_title(\"Title for first plot\")\n",
    "    ref.set_title('Lon='+str(np.round(tide_gauge_name.lon.values,3))+' , '+'Lat='+str(np.round(tide_gauge_name.lat.values,3)))\n",
    "    if counter == 0 :\n",
    "        fig.legend(bbox_to_anchor =(0.1, 0.95), ncol = 3,loc='upper left')\n",
    "    \n",
    "\n",
    "    \n",
    "    #Correlation Analysis\n",
    "\n",
    "    #HIGH RES\n",
    "\n",
    "    data = [alti[\"sla_predicted\"], cmems[\"sla_predicted\"], tg[\"sla_dac\"]]\n",
    "\n",
    "    headers = [\"ML\", \"cmems\", \"TG\"]\n",
    "\n",
    "    df = pd.concat(data, axis=1, keys=headers)\n",
    "\n",
    "    correlation_df = df.corr()\n",
    "    print(tide_gauge_name)\n",
    "    print(correlation_df)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"RMSE ML TG\")\n",
    "    print(np.sqrt(((alti[\"sla_predicted\"] - tg[\"sla_dac\"]) ** 2).mean()))\n",
    "          \n",
    "    print(\"RMSE CMEMS TG\")\n",
    "    print(np.sqrt(((cmems[\"sla_predicted\"] - tg[\"sla_dac\"]) ** 2).mean()))          \n",
    "   \n",
    "    corr_ML[count_TG]=correlation_df.values[0,3]\n",
    "    corr_cmems[count_TG]=correlation_df.values[2,3]\n",
    "    \n",
    "    rms_ML[count_TG]=np.sqrt(((alti[\"sla_predicted\"] - tg[\"sla_dac\"]) ** 2).mean())\n",
    "    rms_cmems[count_TG]=np.sqrt(((cmems[\"sla_predicted\"] - tg[\"sla_dac\"]) ** 2).mean())   \n",
    "    \n",
    "    std_ML[count_TG]=scipy.stats.iqr(alti[\"sla_predicted\"],nan_policy='omit')#np.rms(alti[\"sla_predicted\"]) \n",
    "    std_cmems[count_TG]=  scipy.stats.iqr(cmems[\"sla_predicted\"],nan_policy='omit')#np.rms(cmems[\"sla_predicted\"])\n",
    "    std_tg[count_TG]=  scipy.stats.iqr(tg[\"sla_dac\"],nan_policy='omit')#np.rms(tg[\"sla_dac\"])    \n",
    "    \n",
    "    counter = counter + 1\n",
    "    \n",
    "    #break\n",
    "\n",
    "fig.savefig('fig_timeseriesexample_'+year+'.png')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_names = ['landsortnorra-010-sweden-smhi','gedser-gedser-denmark-dmi',\\\n",
    "#               'visby-026-sweden-smhi']\n",
    "\n",
    "#list_names = ['skanor-019-sweden-smhi','kungsholmsfort-008-sweden-smhi',\\\n",
    "#              'simrishamn-017-sweden-smhi']\n",
    "\n",
    "list_names = ['skanor-019-sweden-smhi']\n",
    "\n",
    "corr_ML=np.empty(np.size(tg_dac.x))*np.nan\n",
    "corr_cmems=np.empty(np.size(tg_dac.x))*np.nan\n",
    "\n",
    "rms_ML=np.empty(np.size(tg_dac.x))*np.nan\n",
    "rms_cmems=np.empty(np.size(tg_dac.x))*np.nan\n",
    "\n",
    "std_ML=np.empty(np.size(tg_dac.x))*np.nan\n",
    "std_cmems=np.empty(np.size(tg_dac.x))*np.nan\n",
    "std_tg=np.empty(np.size(tg_dac.x))*np.nan\n",
    "\n",
    "\n",
    "row_index = np.array([0,0,0])\n",
    "#col_index = np.array([0,1,2])\n",
    "col_index = np.arange(0,np.size(tg_dac.x))\n",
    "\n",
    "fig, axs = plt.subplots(np.size(tg_dac.x), 1, sharex=True, sharey=False, figsize=(10,100))\n",
    "\n",
    "#fig_b, axs_b = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(12,12))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "\n",
    "#for tide_gauge_name in list_names :\n",
    "\n",
    "for count_TG in tg_dac.x[0:] :\n",
    "\n",
    "    #radius_of_distance_km = 100\n",
    "    #tide_gauge_name='visby-026-sweden-smhi'\n",
    "    \n",
    "    start_lat = np.asarray(tg_dac.lat[count_TG].values,dtype=float)\n",
    "    start_lon = np.asarray(tg_dac.lon[count_TG].values,dtype=float)\n",
    "\n",
    "#     if region in {'BALTIC'} :\n",
    "#         tide_gauge_name = tg_dac.name[count_TG]\n",
    "#         tg_due = tg_dac.where(tg_dac.name==tide_gauge_name, drop=True)\n",
    "#         tide_gauge_name = tg_dac.name[count_TG]\n",
    "#     else:\n",
    "    tide_gauge_name = tg_dac.index[count_TG]\n",
    "    tg_due = tg_dac\n",
    "    \n",
    "\n",
    "    ## TIDE CORRECTION\n",
    "\n",
    "    # dates = []\n",
    "\n",
    "    # for j in range(-1,24*28*365+5):\n",
    "    #      dates.append(np.array([datetime(1993, 1, 1, 0, 0, 0) + \n",
    "    # timedelta(hours=1+j)]))\n",
    "\n",
    "    # dates = np.ravel(dates)\n",
    "\n",
    "    # plt.figure(figsize=(15,5),dpi=320)\n",
    "\n",
    "    # tide_correction_fes = pd.read_csv(\"/DGFI8/H/work_marcello/giussani_machinelearning_data/TG/landsortnorra-010-sweden-smhi_fes_ocean.txt\",header=None)/100\n",
    "\n",
    "    # plt.plot(dates,tide_correction_fes,)\n",
    "\n",
    "    # plt.plot(tg_due.date_time,tg_due.sealevel-tide_correction_fes)\n",
    "    # plt.ylabel('m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # initialise data of lists.\n",
    "#     if region in {'BALTIC'} :\n",
    "#         data = {'time':tg_due.time.values,\n",
    "#                 'sla_dac':np.squeeze(tg_due.sla_dac.values)}\n",
    "#     else:\n",
    "    data = {'time':tg_due.time.values,\n",
    "            'sla_dac':tg_due.sla[:,count_TG]}    \n",
    "    #break\n",
    "\n",
    "    # Create DataFrame\n",
    "    tg = pd.DataFrame(data)\n",
    "    \n",
    "    # Remove mean from the tide gauge\n",
    "    mean_tg_due= tg[\"sla_dac\"].mean()\n",
    "    tg.sla_dac = tg.sla_dac - tg[\"sla_dac\"].mean()    \n",
    "    median_tg_due= tg[\"sla_dac\"].median()\n",
    "    mad_tg_due= tg[\"sla_dac\"].mad()    \n",
    "    \n",
    "    # Remove outliers from the tide gauge    \n",
    "    tg.loc[tg[\"sla_dac\"] > median_tg_due+1.4826*2*mad_tg_due] = np.nan\n",
    "    tg.loc[tg[\"sla_dac\"] < median_tg_due-1.4826*2*mad_tg_due] = np.nan\n",
    "    \n",
    "    tg=tg.set_index(tg.time)\n",
    "    tg=tg.loc[year+'-01-01':year+'-12-31']\n",
    "    \n",
    "    tg = tg.groupby(pd.Grouper(freq='24H')).mean()\n",
    "\n",
    "    #means_tg = tg.groupby(pd.Grouper(freq='3H')).mean()\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    #alti\n",
    "    alti=alti_full.loc[year+'-01-15':year+'-12-15']\n",
    "    #alti_only2004=alti_only2004.loc[year+'-11-20':year+'-11-29']\n",
    "    cmems=cmems_full.loc[year+'-01-15':year+'-12-15']\n",
    "    \n",
    "    \n",
    "\n",
    "    # Compute distances from tide gauges \n",
    "    \n",
    "    distances_km = haversine_distance(start_lat, start_lon, alti.lat, alti.lon)\n",
    "    #distances_km_only2004 = haversine_distance(start_lat, start_lon, alti_only2004.lat, alti_only2004.lon)\n",
    "    distances_km_cmems = haversine_distance(start_lat, start_lon, cmems.lat, cmems.lon)\n",
    "\n",
    "    alti['distances_km']=distances_km\n",
    "    #alti_only2004['distances_km']=distances_km\n",
    "    cmems['distances_km']=distances_km_cmems\n",
    "\n",
    "    alti.dropna(subset = [\"sla_predicted\"], inplace=True)\n",
    "    cmems.dropna(subset = [\"sla_predicted\"], inplace=True)\n",
    "    \n",
    "\n",
    "    #alti = alti[distances_km<radius_of_distance_km]\n",
    "    #cmems = cmems[distances_km_cmems<radius_of_distance_km]\n",
    "    \n",
    "    \n",
    "    alti = alti.loc[alti['distances_km'] == np.nanmin(alti['distances_km'])]\n",
    "    cmems = cmems.loc[cmems['distances_km'] == np.nanmin(cmems['distances_km'])]\n",
    "    #cmems = cmems.loc[np.abs(cmems['distances_km'] - np.nanmin(alti['distances_km']))<5]\n",
    "    \n",
    "    \n",
    "\n",
    "#     alti = alti.loc[alti['distances_km'] <20]\n",
    "#     cmems = cmems.loc[cmems['distances_km'] <20]\n",
    " \n",
    "    \n",
    "    #alti = alti.iloc[[np.nanargmin(distances_km)]]\n",
    "    #cmems = cmems.iloc[[np.nanargmin(distances_km_cmems)]]\n",
    "\n",
    "\n",
    "    alti = alti.groupby(['time']).mean()\n",
    "    cmems = cmems.groupby(['time']).mean()\n",
    "    \n",
    "    \n",
    "    #Check that time series are not too short and that they have a similar number of data\n",
    "    if np.shape(alti)[0] < 120 or np.shape(cmems)[0] < 120 or np.shape(tg.sla_dac)[0]<120 or np.shape(tg.sla_dac.values[np.isnan(tg.sla_dac.values)])[0] > 60  :\n",
    "        if counter == 0 :\n",
    "            fig.legend()        \n",
    "        counter = counter + 1\n",
    "        continue\n",
    "        \n",
    "    if np.shape(tg.sla_dac.values[np.isnan(tg.sla_dac.values)])[0] > 60  :\n",
    "        if counter == 0 :\n",
    "            fig.legend()        \n",
    "        counter = counter + 1\n",
    "        continue        \n",
    "    \n",
    "    \n",
    "    freq = 1./10  # Hours\n",
    "    window_size = 60\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # LOESS FILTER IN TIME\n",
    "#     lowess_sm = sm.nonparametric.lowess\n",
    "#     points_to_filter = 10 #100 points along-track correspond to about 60 km\n",
    "#     frac_loess = points_to_filter/np.size(alti.sla_predicted)\n",
    "#     alti.sla_predicted = lowess_sm(np.asarray(alti.sla_predicted),np.asarray(alti.times),frac=frac_loess,it=3, return_sorted = False,missing='drop')\n",
    "#     cmems.sla_predicted = lowess_sm(np.asarray(cmems.sla_predicted),np.asarray(cmems.times),frac=frac_loess,it=3, return_sorted = False,missing='drop')\n",
    "    \n",
    "\n",
    "    # Oversample daily cmems to hourly values\n",
    "    #cmems_oversample = cmems\n",
    "    #cmems_oversample = cmems.resample('3H').pad()\n",
    "    \n",
    "    \n",
    "#     means_alti = alti.groupby(pd.Grouper(freq='24H')).mean()\n",
    "#     means_alti.sla_predicted = np.convolve(means_alti.sla_predicted, np.ones(7)/7, mode='same')\n",
    "    \n",
    "#     means_cmems  = cmems.groupby(pd.Grouper(freq='24H')).mean()\n",
    "#     means_tg = tg.groupby(pd.Grouper(freq='24H')).mean()    \n",
    "    \n",
    "\n",
    "    # Plot of the products\n",
    "    try:\n",
    "        ref = alti.sla_predicted.plot(ax=axs[col_index[counter]],label=\"ML\",title=tide_gauge_name.values,color='blue')\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        cmems.sla_predicted.plot(ax=axs[col_index[counter]],label=\"cmems altimetry\",color='orange')\n",
    "    except:\n",
    "        pass\n",
    "    tg.sla_dac.plot(ax=axs[col_index[counter]],label=\"tide gauge\",color='green')  \n",
    "    ref.set_ylabel(\"TWLA\")\n",
    "    if counter == 0 :\n",
    "        fig.legend()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Plot of the daily products\n",
    "#     means_alti.sla_predicted.plot(ax=axs[row_index[counter],col_index[counter]],label=\"ML\")\n",
    "#     means_cmems.sla_predicted.plot(ax=axs[row_index[counter],col_index[counter]],label=\"cmems altimetry\")\n",
    "#     means_tg.sealevel.plot(ax=axs[row_index[counter],col_index[counter]],label=\"tide gauge\")\n",
    "#     if counter == 0 :\n",
    "#         fig.legend()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Correlation Analysis\n",
    "\n",
    "    #HIGH RES\n",
    "\n",
    "    data = [alti[\"sla_predicted\"], cmems[\"sla_predicted\"], tg[\"sla_dac\"]]\n",
    "\n",
    "    headers = [\"ML\", \"cmems\", \"TG\"]\n",
    "\n",
    "    df = pd.concat(data, axis=1, keys=headers)\n",
    "\n",
    "    #df = {'alti': np.asarray(alti.sla_predicted), 'copernicus': np.asarray(copernicus.sla_predicted), 'tg': np.asarray(tg.sealevel)}\n",
    "    correlation_df = df.corr()\n",
    "    print(tide_gauge_name)\n",
    "    print(correlation_df)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"RMSE ML TG\")\n",
    "    print(np.sqrt(((alti[\"sla_predicted\"] - tg[\"sla_dac\"]) ** 2).mean()))\n",
    "          \n",
    "    print(\"RMSE CMEMS TG\")\n",
    "    print(np.sqrt(((cmems[\"sla_predicted\"] - tg[\"sla_dac\"]) ** 2).mean()))          \n",
    "   \n",
    "    corr_ML[count_TG]=correlation_df.values[0,3]\n",
    "    corr_cmems[count_TG]=correlation_df.values[2,3]\n",
    "    \n",
    "    rms_ML[count_TG]=np.sqrt(((alti[\"sla_predicted\"] - tg[\"sla_dac\"]) ** 2).mean())\n",
    "    rms_cmems[count_TG]=np.sqrt(((cmems[\"sla_predicted\"] - tg[\"sla_dac\"]) ** 2).mean())  \n",
    "    \n",
    "#     stddiff_ML[count_TG]=np.std(alti[\"sla_predicted\"]) - np.std(tg[\"sla_dac\"]) \n",
    "#     stddiff_cmems[count_TG]=  np.std(cmems[\"sla_predicted\"]) - np.std(tg[\"sla_dac\"])    \n",
    "    \n",
    "    std_ML[count_TG]=scipy.stats.iqr(alti[\"sla_predicted\"],nan_policy='omit')#np.rms(alti[\"sla_predicted\"]) \n",
    "    std_cmems[count_TG]=  scipy.stats.iqr(cmems[\"sla_predicted\"],nan_policy='omit')#np.rms(cmems[\"sla_predicted\"])\n",
    "    std_tg[count_TG]=  scipy.stats.iqr(tg[\"sla_dac\"],nan_policy='omit')#np.rms(tg[\"sla_dac\"])    \n",
    "    \n",
    "    counter = counter + 1\n",
    "    \n",
    "    #break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO PLOT SINGLE INSTANTANEOUS MAPS -  to be externalised\n",
    "\n",
    "def single_plot(rms_cmemsvsML_plot,min_sl,max_sl,counter,row_total,col_total,fig,min_lat,max_lat,min_lon,max_lon,label_string,colormap):\n",
    "\n",
    "    lon = rms_cmemsvsML_plot[0,:]\n",
    "    lat = rms_cmemsvsML_plot[1,:]\n",
    "    ssh = rms_cmemsvsML_plot[2,:]   \n",
    "    \n",
    "    \"\"\"Displaying the unstructured grid in a scatterplot:\"\"\"\n",
    "    # \n",
    "\n",
    "    #fig = plt.plot()\n",
    "    #plt.rcParams.update({'font.size': 15})\n",
    "    #plt.plot\n",
    "    #plt.rcParams[\"figure.figsize\"] = (50,10) #Increase figure size\n",
    "    ax=fig.add_subplot(row_total,col_total,counter,projection=ccrs.PlateCarree())\n",
    "    \n",
    "    ax.set_xticks(np.arange(min_lon,max_lon,2), crs=ccrs.PlateCarree())\n",
    "    ax.set_yticks(np.arange(min_lat,max_lat,1), crs=ccrs.PlateCarree())    \n",
    "    \n",
    "    #ax = plt.axes(projection=ccrs.Miller()) \n",
    "    img=plt.scatter(lon, lat, c=ssh, s=80, cmap=colormap, alpha=1) \n",
    "    ax.coastlines(resolution='10m', color='black', linewidth=1) \n",
    "    plt.xlim(min_lon,max_lon)\n",
    "\n",
    "    lon_formatter = cart.mpl.ticker.LongitudeFormatter(number_format='.1f', \n",
    "                                      degree_symbol='', \n",
    "                                      dateline_direction_label=True)\n",
    "    lat_formatter = cart.mpl.ticker.LatitudeFormatter(number_format='.1f', \n",
    "                                     degree_symbol='')\n",
    "    ax.xaxis.set_major_formatter(lon_formatter)\n",
    "    ax.yaxis.set_major_formatter(lat_formatter) \n",
    "    plt.colorbar(img,label=label_string,orientation = 'horizontal',pad = 0.1)\n",
    "    plt.clim(min_sl, max_sl)\n",
    "    \n",
    "    ax.set_extent([min_lon,max_lon,min_lat,max_lat,])\n",
    "    #plt.show() #a window will show up, allowing the user to see and download the plot\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Corr and RMS of ML\n",
    "rms_ML_plot=np.vstack([np.asarray(tg_dac.lon.values,dtype=float),np.asarray(tg_dac.lat.values,dtype=float),rms_ML,])\n",
    "corr_ML_plot=np.vstack([np.asarray(tg_dac.lon.values,dtype=float),np.asarray(tg_dac.lat.values,dtype=float),corr_ML,])\n",
    "\n",
    "\n",
    "# Absolute differences in RMS, Correlation and standard deviation\n",
    "rms_cmemsvsML_plot=np.vstack([np.asarray(tg_dac.lon.values,dtype=float),np.asarray(tg_dac.lat.values,dtype=float),rms_cmems-rms_ML,])\n",
    "corr_cmemsvsML_plot=np.vstack([np.asarray(tg_dac.lon.values,dtype=float),np.asarray(tg_dac.lat.values,dtype=float),corr_cmems-corr_ML,])\n",
    "\n",
    "# Percentage differences in RMS and Correlation\n",
    "rms_pc_cmemsvsML_plot=np.vstack([np.asarray(tg_dac.lon.values,dtype=float),np.asarray(tg_dac.lat.values,dtype=float),((rms_cmems-rms_ML)/rms_ML)*100,])\n",
    "corr_pc_cmemsvsML_plot=np.vstack([np.asarray(tg_dac.lon.values,dtype=float),np.asarray(tg_dac.lat.values,dtype=float),((corr_cmems-corr_ML)/corr_ML)*100,])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (15, 10))\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "\n",
    "min_sl = 0.01\n",
    "max_sl = 0.13\n",
    "single_plot(rms_ML_plot,min_sl,max_sl,1,2,2,fig,min_lat,max_lat,min_lon,max_lon,r'RMS ML (m)','cool')\n",
    "plt.title('(a)')\n",
    "\n",
    "min_sl = 0.4\n",
    "max_sl = 1.0\n",
    "single_plot(corr_ML_plot,min_sl,max_sl,2,2,2,fig,min_lat,max_lat,min_lon,max_lon,r'CORR ML','cool')\n",
    "plt.title('(b)')\n",
    "\n",
    "# min_sl = -0.01\n",
    "# max_sl = 0.01\n",
    "# single_plot(rms_cmemsvsML_plot,min_sl,max_sl,3,3,2,fig,min_lat,max_lat,min_lon,max_lon,r'RMS CMEMS-ML (m)','bwr')\n",
    "\n",
    "min_sl = -20\n",
    "max_sl = 20\n",
    "single_plot(rms_pc_cmemsvsML_plot,min_sl,max_sl,3,2,2,fig,min_lat,max_lat,min_lon,max_lon,r'RMS % CMEMS-ML','bwr')\n",
    "plt.title('(c)')\n",
    "\n",
    "\n",
    "if region in {'BALTIC'} :\n",
    "    min_sl = -0.05\n",
    "    max_sl = 0.05\n",
    "elif region in {'NORTHSEA'} :  \n",
    "    min_sl = -0.15\n",
    "    max_sl = 0.15    \n",
    "    \n",
    "single_plot(corr_cmemsvsML_plot,min_sl,max_sl,4,2,2,fig,min_lat,max_lat,min_lon,max_lon,r'CORR CMEMS-ML','bwr_r')\n",
    "plt.title('(d)')\n",
    "\n",
    "\n",
    "# if region in {'BALTIC'} :\n",
    "#     min_sl = -5\n",
    "#     max_sl = 5\n",
    "# elif region in {'NORTHSEA'} :\n",
    "#     min_sl = -15\n",
    "#     max_sl = 15\n",
    "# single_plot(corr_pc_cmemsvsML_plot,min_sl,max_sl,6,3,2,fig,min_lat,max_lat,min_lon,max_lon,r'CORR % CMEMS-ML','bwr_r')\n",
    "\n",
    "\n",
    "# Correlation\n",
    "print('Mean difference in correlation ML - CMEMS (%)')\n",
    "print(np.nanmean((corr_ML - corr_cmems) / corr_cmems * 100))\n",
    "\n",
    "# RMS\n",
    "print('Mean difference in RMS ML - CMEMS (%)')\n",
    "print(np.nanmean((rms_ML - rms_cmems) / rms_cmems *100))\n",
    "\n",
    "fig.savefig('fig_corr_rms_'+year+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Deviations\n",
    "\n",
    "stddiff_ML_plot=np.vstack([np.asarray(tg_dac.lon.values,dtype=float),np.asarray(tg_dac.lat.values,dtype=float),(std_ML-std_tg)/std_tg *100,])\n",
    "\n",
    "stddiff_cmems_plot=np.vstack([np.asarray(tg_dac.lon.values,dtype=float),np.asarray(tg_dac.lat.values,dtype=float),(std_cmems-std_tg)/std_tg *100,])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (15, 10))\n",
    "\n",
    "min_sl = -10\n",
    "max_sl = 10\n",
    "single_plot(stddiff_ML_plot ,min_sl,max_sl,1,1,2,fig,min_lat,max_lat,min_lon,max_lon,r'std ML - std TG (%)','bwr')\n",
    "\n",
    "\n",
    "single_plot(stddiff_cmems_plot,min_sl,max_sl,2,1,2,fig,min_lat,max_lat,min_lon,max_lon,r'std CMEMS - std TG (%)','bwr')\n",
    "\n",
    "\n",
    "print('Average misrepresented signal in ML (%)')\n",
    "print(np.nanmean( (std_ML-std_tg)/std_tg *100)) \n",
    "\n",
    "print('Average misrepresented signal in CMEMS (%)')\n",
    "print(np.nanmean( (std_cmems-std_tg)/std_tg *100) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the variability for use in other notebook\n",
    "import pickle as pl\n",
    "pl.dump(std_tg,file=open('std_tg_'+region+'_'+year+'.pickle','wb'))\n",
    "pl.dump(std_ML,file=open('std_ML_'+region+'_'+year+'.pickle','wb'))\n",
    "pl.dump(std_cmems,file=open('std_cmems_'+region+'_'+year+'.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
